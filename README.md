# eestech-hackathon
# Hackathon Submission: Customer Experience Enhancement with Public Data and Generative AI

## Introduction
This repository contains the submission of Team 10 for the Hackathon organized by EESTEC LC Aachen in partnership with Infineon. Our project aims to enhance customer experience using public data and generative AI.

## Problem Statement
The topic of the hackathon is "How can we improve customer experience with public data and generative AI?". Our team proposes a solution using a Large Language Model (LLM)-based model. The model provides feedback to different personas, acting as customers, by analyzing their comments and suggesting improvements. More deatils on problem statement can be found at [github](https://github.com/Infineon/hackathon)

## Solution Overview
Our solution is based on a Large Language Model (LLM) that analyzes customer comments and provides tailored feedback to improve their experience. We have identified and defined customer experience metrics, both abstract and non-abstract, by considering various customer personas and viewpoints. By leveraging public data and generative AI, our model offers effective and consistent predictions on how issues and comments can be handled better to enhance customer satisfaction. 

## Key Features
- Language Model (LLM)-based solution
- Feedback generation for different customer personas and the end-user customer.
- Definition and analysis of abstract and non-abstract customer experience metrics.
- Leveraging public data and generative AI for enhanced predictions.

## How It Works
1. **Data Collection**: We gather customer comments and related data from public sources. For our current dummy implementation we used the pickle data of github issues (with user comments). 
2. **Preprocessing**: The data is preprocessed to remove noise and irrelevant information. We identified that some of the entries in the master pickle file comprised of Bot or automated github pull-requests and pull-requests in general which we processed as not relevant to the task at hand and so were discarded due to the fact that any inference derieved from such issues would not contribute to improving customer experience. The details on precprocessing including discussions can be found in the following python notebook `data-preprocessing.ipynb`. Some information was observed during preprocessing, mainly the fact that we needed more information about user comments to get more context for us and as an input to the LLM for better understanding of the history of an issue, to this purpose edits were made in `fetch_github_issues.py` to gather information about comments and more information over issues.   
3. **Model Selection**: We selected the Language Model (LLM) with respect to the computational and compatibility requirements of the task at hand. It was noted that although real time behaviour is not required, the model should process eaxh isuue within 10 minutes of standard time in a normal PC equipped with 8GB CPU RAM. We used a na√ève chat-based state-of-the-art LLM since it can be remarked that the train data was not enough to notice any change in the model's output or performance.  
4. **Feedback Generation**: The model analyzes customer comments and generates feedback tailored to different personas. Another key feature of the model is to provide different feedback to different personas using them, after consideration of their responsibilities, goals etc.
5. **Evaluation Metric Definition**: Another key endevour in the project is to define customer experience. We considered customer experience with respect to different personas within the organisation and the customer (end-user) as well, outside the organisation. Thus many numerable and abstract metrics.
6. **Evaluation**: We evaluate the effectiveness of the feedback generated by the model using various custom-defined metrics.

## Customer Experience Definition
The tool was to be used by 4 users of different personas, Harald, Julian, Daniel and Sarah, as an example. These users and many other part of the company used the tool in order to get feedback to ultimately better the customer experience. So, we put us into their roles and also to the shoes of the customer and identified the following parameters to be used as performance metrics by our LLM.
- Response Time: calculate the time between creation of the issue and the first comment to the issue and evaluate if that is reasonable with respect to the issue.
- Resolution Time: calculate the time between creation of the issue and the resolution to the issue and evaluate if that is reasonable with respect to the issue.
- Response Effectiveness: read the comments and the sequence of comments to the issue and evaluate if that helps the customer or in resolution of the issue.
- Empathy and Friendliness: read the tone of users' comments to the issue and evaluate if the comments are valid, helpful and unbaised especially with respect to knowledge level and experience of the user.
- Customer Satisfaction: read customer comments (if any) and evaluate if the activity in the issue was helpful to the customer. 

The parameters were identified as being the most important to the end-user, the customer and are listed in no particular order. These metrics were used in the prompt engineering and the LLM should consider them while making comments on the user comments to better customer experience.

## Prompt Engineering
Prompt engineering involves crafting specific prompts or instructions given to a language model to produce desired outputs. The goal of this exercise is to guide the model towards generating outputs that align with the intended objective. The focus on Prompt Engineering is to ask the correct questions and getting the precise and homogenous answers and parsing them in an effectively readable and understandble format. Following are some examples data where we brainstormed for the correct prompt while prompt training. <examples if possible>

## Getting Started
To get started with our project, follow these steps:
1. Clone this repository to your local machine.
2. Install the required dependencies <as mentioned in the `requirements.txt` file.>
3. Collect and prepare the required data to send to the preprocessing step. Here we can correspond it to `github_issues.pkl`.
4. Preprocess the data by identifying its patterns and its redundancies and remove the extra information. Here we can correspond to `fetch_github_issues.py`.
5. Work on your prompt, engineer it, perform trial-and-error iterations with `my-llama-example.py` till you get satisfactory results.
6. Your model and prompt is ready to be deployed in the UI.

## Project Outcome Overview
We have dereived a correct prompt with post-processing or parsing of the result such as to obtain a robust result output from the LLM which can be used by internal users so as to improve the customer experience.

## Future Improvement Outlook
As stated with more time duration, the team could have worked more on getting the data from the Infineon User Community using a web scrapper. A dummy version was tried and the title of the issues was obtained but it was observed that to gather all the information was too complex and time consuming so the endeavour was abandoned.

Another area where focus was left out was the comparision and implementation of different LLMs and their finetuning over the input data. We remarked that such a small amount of data would not create an appreciable change in the internet scaled trained state of the art LLMs used. Also, we did glance over the hyperparameters of the model selected but that was too forgone over other priority issues like prompt engineering.

We started working on the UI on the final day, and would like to improve much on this rudimentary UI.

## Contributors
Team 10 EESTEC LC Aachen AI Hackathon 2024
- [Manas Mehrotra](https://github.com/mechgguy)
- [Kh Safkat Amin](https://github.com/khsafkatamin)
- [Team Member 3](https://github.com/teammember3)

Endeavour overtaken between the 03.05.2024-05.05.2024

## Acknowledgements
We would like to thank EESTEC LC Aachen and Infineon for organizing this hackathon and providing us with the opportunity to work on such an exciting project.

## License
This project is licensed under the [MIT License](https://www.mit.edu/~amini/LICENSE.md).
